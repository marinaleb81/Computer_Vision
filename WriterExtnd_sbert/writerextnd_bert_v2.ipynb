{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 1: Импорты и основные настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Все необходимые библиотеки импортированы.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Закомментированы, так как не используются в текущей версии кода:\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from collections import Counter\n",
    "import nltk # Используется для разбиения на предложения\n",
    "\n",
    "print(\"Все необходимые библиотеки импортированы.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 2: Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Конфигурация авторов и ID проверена.\n",
      "Модель SBERT: sberbank-ai/sbert_large_nlu_ru\n",
      "Папка с данными: E:\\cv_project\\writerextnd\n",
      "Предложений в чанке: 10\n",
      "Авторы для обучения: ['Bradbury', 'Genri', 'Simak', 'Bulgakov', 'Strugatskie', 'Fry']\n"
     ]
    }
   ],
   "source": [
    "# --- Модель и Данные ---\n",
    "MODEL_NAME = 'sberbank-ai/sbert_large_nlu_ru'  # Имя модели SBERT\n",
    "data_dir = Path('E:/cv_project/writerextnd/') # !!! УБЕДИТЕСЬ, ЧТО ПУТЬ ПРАВИЛЬНЫЙ !!!\n",
    "SENTENCES_PER_CHUNK = 10 # Количество предложений в одном чанке для обучения\n",
    "\n",
    "# --- Обучающие данные: Имена авторов -> Файлы ---\n",
    "training_files = {\n",
    "    'Bradbury': 'Bradbury.txt',\n",
    "    'Genri': 'Genri.txt',\n",
    "    'Simak': 'Simak.txt',\n",
    "    'Bulgakov': 'Bulgakov.txt',\n",
    "    'Strugatskie': 'Strugatskie.txt',\n",
    "    'Fry': 'Fry.txt'\n",
    "}\n",
    "\n",
    "# --- Карта имен авторов к их числовым ID ---\n",
    "author_to_id_map = {\n",
    "    'Bradbury': 3,\n",
    "    'Genri': 0,\n",
    "    'Simak': 1,\n",
    "    'Bulgakov': 2,\n",
    "    'Strugatskie': 5,\n",
    "    'Fry': 4\n",
    "}\n",
    "\n",
    "# --- Проверка соответствия имен авторов и ID ---\n",
    "missing_authors = [author for author in training_files if author not in author_to_id_map]\n",
    "if missing_authors:\n",
    "    raise ValueError(f\"Ошибка: Не найдены ID для следующих авторов в author_to_id_map: {missing_authors}\")\n",
    "else:\n",
    "    print(\"Конфигурация авторов и ID проверена.\")\n",
    "\n",
    "print(f\"Модель SBERT: {MODEL_NAME}\")\n",
    "print(f\"Папка с данными: {data_dir}\")\n",
    "print(f\"Предложений в чанке: {SENTENCES_PER_CHUNK}\")\n",
    "print(f\"Авторы для обучения: {list(training_files.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 3: Проверка и загрузка NLTK данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK пакет 'punkt' для токенизации предложений найден.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK пакет 'punkt' для токенизации предложений найден.\")\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Загрузка пакета 'punkt' для nltk...\")\n",
    "    nltk.download('punkt')\n",
    "    print(\"Пакет 'punkt' загружен.\")\n",
    "except Exception as e:\n",
    "    print(f\"Не удалось проверить/загрузить nltk 'punkt': {e}\")\n",
    "    print(\"Попробуйте установить вручную: python -m nltk.downloader punkt\")\n",
    "    # exit() # Если автоматическая загрузка не удалась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 5: Загрузка и нарезка обучающих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Чтение и нарезка обучающих данных из папки: E:\\cv_project\\writerextnd\n",
      "Создание чанков по 10 предложений.\n",
      " - Обработка файла: Bradbury.txt (Автор: Bradbury, ID: 3)\n",
      "   - Найдено предложений: 23934\n",
      "   - Создано чанков: 2394\n",
      " - Обработка файла: Genri.txt (Автор: Genri, ID: 0)\n",
      "   - Найдено предложений: 12668\n",
      "   - Создано чанков: 1267\n",
      " - Обработка файла: Simak.txt (Автор: Simak, ID: 1)\n",
      "   - Найдено предложений: 24444\n",
      "   - Создано чанков: 2445\n",
      " - Обработка файла: Bulgakov.txt (Автор: Bulgakov, ID: 2)\n",
      "   - Найдено предложений: 21268\n",
      "   - Создано чанков: 2127\n",
      " - Обработка файла: Strugatskie.txt (Автор: Strugatskie, ID: 5)\n",
      "   - Найдено предложений: 29589\n",
      "   - Создано чанков: 2959\n",
      " - Обработка файла: Fry.txt (Автор: Fry, ID: 4)\n",
      "   - Найдено предложений: 53640\n",
      "   - Создано чанков: 5364\n",
      "\n",
      "Всего создано обучающих чанков: 16556\n"
     ]
    }
   ],
   "source": [
    "chunked_train_texts = []      # Список для хранения текстовых чанков\n",
    "chunked_train_author_ids = [] # Список для хранения ID авторов для каждого чанка\n",
    "errors_loading_train = []     # Список для ошибок при загрузке обучения\n",
    "\n",
    "print(f\"\\nЧтение и нарезка обучающих данных из папки: {data_dir}\")\n",
    "print(f\"Создание чанков по {SENTENCES_PER_CHUNK} предложений.\")\n",
    "\n",
    "total_chunks_created = 0\n",
    "for author, filename in training_files.items():\n",
    "    file_path = data_dir / filename\n",
    "    author_id = author_to_id_map[author]\n",
    "    chunks_for_author = 0\n",
    "    try:\n",
    "        if file_path.is_file():\n",
    "            print(f\" - Обработка файла: {filename} (Автор: {author}, ID: {author_id})\")\n",
    "            text = file_path.read_text(encoding='utf-8')\n",
    "            if not text.strip():\n",
    "                print(f\"   - Предупреждение: Файл {filename} пустой.\")\n",
    "                continue # Переходим к следующему файлу\n",
    "\n",
    "            # Разбиваем текст на предложения с указанием языка\n",
    "            sentences = nltk.sent_tokenize(text, language='russian')\n",
    "            print(f\"   - Найдено предложений: {len(sentences)}\")\n",
    "\n",
    "            # Делим предложения на чанки\n",
    "            for i in range(0, len(sentences), SENTENCES_PER_CHUNK):\n",
    "                chunk_sentences = sentences[i : i + SENTENCES_PER_CHUNK]\n",
    "                chunk_text = \" \".join(chunk_sentences)\n",
    "                # Добавляем только непустые чанки\n",
    "                if chunk_text.strip():\n",
    "                    chunked_train_texts.append(chunk_text)\n",
    "                    chunked_train_author_ids.append(author_id)\n",
    "                    chunks_for_author += 1\n",
    "\n",
    "            print(f\"   - Создано чанков: {chunks_for_author}\")\n",
    "            total_chunks_created += chunks_for_author\n",
    "        else:\n",
    "            error_msg = f\" - Ошибка: Обучающий файл не найден: {file_path}\"\n",
    "            print(error_msg)\n",
    "            errors_loading_train.append(error_msg)\n",
    "    except Exception as e:\n",
    "        error_msg = f\" - Ошибка при чтении/обработке файла {file_path}: {e}\"\n",
    "        print(error_msg)\n",
    "        errors_loading_train.append(error_msg)\n",
    "\n",
    "print(f\"\\nВсего создано обучающих чанков: {total_chunks_created}\")\n",
    "\n",
    "# Проверка, были ли созданы чанки\n",
    "if total_chunks_created == 0:\n",
    "    if not errors_loading_train:\n",
    "        print(\"!!! КРИТИЧЕСКАЯ ОШИБКА: Не было создано ни одного обучающего чанка. Проверьте файлы и параметр SENTENCES_PER_CHUNK.\")\n",
    "    else:\n",
    "         print(\"!!! КРИТИЧЕСКАЯ ОШИБКА: Не было создано ни одного обучающего чанка из-за ошибок чтения файлов.\")\n",
    "    exit() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 6: Загрузка тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Загрузка тестовых текстов (файлы 'author*.txt')...\n",
      " - Тестовые ID отсортированы по числовому значению.\n",
      "   - Загружен author1.txt (ID: author1), символов: 1629\n",
      "   - Загружен author2.txt (ID: author2), символов: 2459\n",
      "   - Загружен author3.txt (ID: author3), символов: 1323\n",
      "   - Загружен author4.txt (ID: author4), символов: 2103\n",
      "   - Загружен author5.txt (ID: author5), символов: 3207\n",
      "   - Загружен author6.txt (ID: author6), символов: 2181\n",
      "   - Загружен author7.txt (ID: author7), символов: 9949\n",
      "   - Загружен author8.txt (ID: author8), символов: 5264\n",
      "   - Загружен author9.txt (ID: author9), символов: 19488\n",
      "   - Загружен author10.txt (ID: author10), символов: 1657\n",
      "   - Загружен author11.txt (ID: author11), символов: 2633\n",
      "   - Загружен author12.txt (ID: author12), символов: 3946\n",
      "   - Загружен author13.txt (ID: author13), символов: 2090\n",
      "   - Загружен author14.txt (ID: author14), символов: 2013\n",
      "   - Загружен author15.txt (ID: author15), символов: 2354\n",
      "   - Загружен author16.txt (ID: author16), символов: 1102\n",
      "   - Загружен author17.txt (ID: author17), символов: 6869\n",
      "   - Загружен author18.txt (ID: author18), символов: 29614\n",
      "   - Загружен author19.txt (ID: author19), символов: 13314\n",
      "   - Загружен author20.txt (ID: author20), символов: 1620\n",
      "   - Загружен author21.txt (ID: author21), символов: 1795\n"
     ]
    }
   ],
   "source": [
    "test_data = {}\n",
    "test_ids_sorted = []\n",
    "errors_loading_test = []\n",
    "\n",
    "print(\"\\nЗагрузка тестовых текстов (файлы 'author*.txt')...\")\n",
    "test_files_paths = sorted(list(data_dir.glob('author*.txt')))\n",
    "\n",
    "if not test_files_paths:\n",
    "      error_msg = f\" - Ошибка: Тестовые файлы ('author*.txt') не найдены в {data_dir}\"\n",
    "      print(error_msg)\n",
    "      errors_loading_test.append(error_msg)\n",
    "else:\n",
    "    try:\n",
    "        test_ids_sorted = sorted(\n",
    "            [fp.stem for fp in test_files_paths],\n",
    "            key=lambda x: int(x.replace('author', ''))\n",
    "        )\n",
    "        print(\" - Тестовые ID отсортированы по числовому значению.\")\n",
    "    except ValueError:\n",
    "        print(\" - Предупреждение: Не удалось отсортировать тестовые ID по числу. Используется стандартная сортировка строк.\")\n",
    "        test_ids_sorted = sorted([fp.stem for fp in test_files_paths])\n",
    "\n",
    "    test_files_paths_map = {fp.stem: fp for fp in test_files_paths}\n",
    "\n",
    "    for file_id in test_ids_sorted:\n",
    "        file_path = test_files_paths_map[file_id]\n",
    "        try:\n",
    "            text = file_path.read_text(encoding='utf-8')\n",
    "            if text.strip():\n",
    "                test_data[file_id] = text\n",
    "                print(f\"   - Загружен {file_path.name} (ID: {file_id}), символов: {len(text)}\")\n",
    "            else:\n",
    "                print(f\"   - Предупреждение: Тестовый файл {file_path.name} пустой, пропускается.\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"   - Ошибка при чтении тестового файла {file_path}: {e}\"\n",
    "            print(error_msg)\n",
    "            errors_loading_test.append(error_msg)\n",
    "\n",
    "test_ids_sorted = [id_ for id_ in test_ids_sorted if id_ in test_data]\n",
    "\n",
    "if not test_data:\n",
    "    print(\" - Предупреждение: Не загружено ни одного тестового текста.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 7: Отчет об ошибках загрузки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errors = errors_loading_train + errors_loading_test\n",
    "if all_errors:\n",
    "    print(\"\\n--- Обнаружены ошибки при загрузке файлов ---\")\n",
    "    for err in all_errors:\n",
    "        print(err)\n",
    "    # print(\"Прерывание выполнения из-за ошибок загрузки.\")\n",
    "    # exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 8: Подготовка данных для модели и проверка распределения классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Подготовлено 16556 обучающих чанков.\n",
      "Подготовлено 21 тестовых текстов для предсказания.\n",
      "\n",
      "Распределение классов в обучении (по чанкам): Counter({4: 5364, 5: 2959, 1: 2445, 3: 2394, 2: 2127, 0: 1267})\n",
      "Минимальное количество образцов (чанков) на класс: 1267\n"
     ]
    }
   ],
   "source": [
    "train_texts = chunked_train_texts\n",
    "train_author_ids = chunked_train_author_ids\n",
    "\n",
    "test_texts = [test_data[id_] for id_ in test_ids_sorted if id_ in test_data]\n",
    "\n",
    "print(f\"\\nПодготовлено {len(train_texts)} обучающих чанков.\")\n",
    "if test_texts:\n",
    "    print(f\"Подготовлено {len(test_texts)} тестовых текстов для предсказания.\")\n",
    "else:\n",
    "    print(\"Тестовые тексты для предсказания отсутствуют или не были загружены.\")\n",
    "\n",
    "class_counts = Counter(train_author_ids)\n",
    "print(f\"\\nРаспределение классов в обучении (по чанкам): {class_counts}\")\n",
    "\n",
    "min_samples_per_class = 0\n",
    "if class_counts:\n",
    "    min_samples_per_class = min(class_counts.values())\n",
    "print(f\"Минимальное количество образцов (чанков) на класс: {min_samples_per_class}\")\n",
    "\n",
    "if min_samples_per_class < 2:\n",
    "    print(\"\\n!!! ПРЕДУПРЕЖДЕНИЕ !!!\")\n",
    "    print(f\"После нарезки у одного или нескольких классов меньше 2 образцов ({min_samples_per_class}).\")\n",
    "    print(\"Стандартная кросс-валидация (5-fold) невозможна.\")\n",
    "    print(\"Попробуйте уменьшить SENTENCES_PER_CHUNK или проверьте объем исходных текстов.\")\n",
    "    if min_samples_per_class < 1:\n",
    "         print(\"!!! КРИТИЧЕСКАЯ ОШИБКА: Есть класс(ы) без единого обучающего образца!\")\n",
    "         exit() # Обучение невозможно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 9: Генерация эмбеддингов для обучающих данных\n",
    "\n",
    "Обучение длится 1 час 45 мин."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Генерация эмбеддингов для 16556 обучающих чанков с помощью sberbank-ai/sbert_large_nlu_ru...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   7%|▋         | 35/518 [22:04<4:24:14, 32.83s/it]"
     ]
    }
   ],
   "source": [
    "print(f\"\\nГенерация эмбеддингов для {len(train_texts)} обучающих чанков с помощью {MODEL_NAME}...\")\n",
    "start_time = time.time()\n",
    "train_embeddings = model_st.encode(\n",
    "    train_texts,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True \n",
    ")\n",
    "end_time = time.time()\n",
    "print(f\"Генерация обучающих эмбеддингов заняла: {end_time - start_time:.2f} сек.\")\n",
    "print(f\"Размер матрицы обучающих эмбеддингов: {train_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 10: Генерация эмбеддингов для тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = np.array([]) # Инициализируем пустым массивом\n",
    "\n",
    "if test_texts:\n",
    "    print(f\"\\nГенерация эмбеддингов для {len(test_texts)} тестовых текстов...\")\n",
    "    start_time = time.time()\n",
    "    test_embeddings = model_st.encode(\n",
    "        test_texts,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    print(f\"Генерация тестовых эмбеддингов заняла: {end_time - start_time:.2f} сек.\")\n",
    "    if test_embeddings.ndim == 2: # Убедимся, что это двумерный массив\n",
    "         print(f\"Размер матрицы тестовых эмбеддингов: {test_embeddings.shape}\")\n",
    "    else:\n",
    "         print(f\"Получен массив эмбеддингов неожиданной размерности: {test_embeddings.shape}\")\n",
    "         test_embeddings = np.array([])\n",
    "else:\n",
    "    print(\"\\nНет тестовых текстов для генерации эмбеддингов.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 11: Обучение классификатора (LinearSVC с GridSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = None # Инициализируем классификатор\n",
    "\n",
    "print(\"\\nОбучение классификатора LinearSVC...\")\n",
    "\n",
    "# param_grid = {'C': [0.1, 1, 10, 100, 1000]} # Лучший результат при С = 10!\n",
    "# param_grid_refined = {'C': [9, 9.5, 10, 10.5, 11]} # попробовать помельче шаг, для уточности\n",
    "\n",
    "param_grid = {\n",
    "    'C': [5, 8, 10, 12, 15, 20], \n",
    "    'loss': ['hinge', 'squared_hinge'] # По умолчанию 'squared_hinge'\n",
    "}\n",
    "\n",
    "base_classifier = LinearSVC(random_state=42, dual=True, max_iter=2000, class_weight='balanced') # dual='auto' с версии 1.2\n",
    "\n",
    "n_splits = min(5, min_samples_per_class) # 5 фолдов, равное числу образцов\n",
    "\n",
    "if n_splits < 2:\n",
    "    print(f\"\\n!!! Невозможно выполнить кросс-валидацию (n_splits={n_splits} < 2).\")\n",
    "    print(\"Обучение будет выполнено без GridSearchCV с параметром C=1.0.\")\n",
    "    classifier = LinearSVC(C=1.0, random_state=42, dual=True, max_iter=2000, class_weight='balanced')\n",
    "    start_time_fit = time.time()\n",
    "    try:\n",
    "        classifier.fit(train_embeddings, train_author_ids)\n",
    "        print(f\"Обучение базового классификатора заняло: {time.time() - start_time_fit:.2f} сек.\")\n",
    "    except Exception as fit_e:\n",
    "        print(f\"!!! КРИТИЧЕСКАЯ ОШИБКА при обучении базового LinearSVC: {fit_e}\")\n",
    "        exit() # Прерываем, если даже базовая модель не обучилась\n",
    "else:\n",
    "    print(f\"Подбор гиперпараметров для LinearSVC с помощью GridSearchCV ({n_splits}-fold CV)...\")\n",
    "    # n_jobs=-1 использует все доступные ядра процессора\n",
    "    # verbose=2 показывает больше информации о процессе поиска\n",
    "    grid_search = GridSearchCV(base_classifier, param_grid, cv=n_splits, n_jobs=-1, verbose=2)\n",
    "\n",
    "    start_time_grid = time.time()\n",
    "    try:\n",
    "        # Обучаем GridSearchCV на обучающих данных\n",
    "        grid_search.fit(train_embeddings, train_author_ids)\n",
    "        print(f\"\\nGridSearchCV завершен за: {time.time() - start_time_grid:.2f} сек.\")\n",
    "\n",
    "        # Получаем лучший классификатор по результатам GridSearchCV\n",
    "        classifier = grid_search.best_estimator_\n",
    "        print(f\"Лучшие параметры найдены: {grid_search.best_params_}\")\n",
    "        print(f\"Лучшая средняя точность на кросс-валидации ({n_splits}-fold): {grid_search.best_score_:.4f}\")\n",
    "        print(\"Классификатор (LinearSVC с лучшими параметрами) успешно обучен.\")\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n!!! ОШИБКА во время GridSearchCV: {e}\")\n",
    "        print(\"Возможная причина: недостаточно данных или проблемы с параметрами.\")\n",
    "        print(\"Попытка обучить модель без GridSearchCV с параметром C=1.0...\")\n",
    "        classifier = LinearSVC(C=1.0, random_state=42, dual=True, max_iter=2000, class_weight='balanced')\n",
    "        start_time_fit_alt = time.time()\n",
    "        try:\n",
    "            classifier.fit(train_embeddings, train_author_ids)\n",
    "            print(f\"Обучение базового классификатора заняло: {time.time() - start_time_fit_alt:.2f} сек.\")\n",
    "        except Exception as fit_e_alt:\n",
    "            print(f\"!!! КРИТИЧЕСКАЯ ОШИБКА при обучении базового LinearSVC после ошибки GridSearchCV: {fit_e_alt}\")\n",
    "            exit() # Прерываем, если и базовая модель не обучилась\n",
    "\n",
    "# Финальная проверка, что классификатор был создан\n",
    "if classifier is None:\n",
    "    print(\"!!! КРИТИЧЕСКАЯ ОШИБКА: Классификатор не был инициализирован или обучен. Выход.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 12: Предсказание на тестовых данных и создание файла submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_embeddings.size > 0 and classifier is not None:\n",
    "    print(\"\\nПолучение предсказаний для тестовых данных...\")\n",
    "    start_time = time.time()\n",
    "    predicted_author_ids = classifier.predict(test_embeddings)\n",
    "    end_time = time.time()\n",
    "    print(f\"Предсказание заняло: {end_time - start_time:.2f} сек.\")\n",
    "    print(f\"Предсказания (числовые ID) получены для {len(predicted_author_ids)} тестовых отрывков.\")\n",
    "\n",
    "    # Вывод примеров предсказаний\n",
    "    print(\"\\nПримеры предсказаний:\")\n",
    "    id_to_author_map = {v: k for k, v in author_to_id_map.items()} # Обратная карта для имен\n",
    "    for i in range(min(5, len(predicted_author_ids))):\n",
    "         pred_id = predicted_author_ids[i]\n",
    "         pred_author = id_to_author_map.get(pred_id, f\"Неизвестный ID {pred_id}\")\n",
    "         print(f\"  ID файла: {test_ids_sorted[i]} -> Предсказан ID: {pred_id} (Автор: {pred_author})\")\n",
    "\n",
    "    # --- Создание DataFrame для submission ---\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids_sorted,      # Используем отсортированные ID\n",
    "        'label': predicted_author_ids # Предсказанные ID\n",
    "    })\n",
    "\n",
    "    # --- Сохранение в CSV ---\n",
    "    submission_filename = \"submission_sbert_chunked_gridsearch_v2.csv\"\n",
    "    submission_path = data_dir / submission_filename # Сохраняем в ту же папку с данными\n",
    "    try:\n",
    "        submission_df.to_csv(submission_path, index=False, sep=',')\n",
    "        print(f\"\\nФайл для отправки '{submission_path.resolve()}' успешно создан.\")\n",
    "        print(\"Содержимое файла (первые 5 строк):\")\n",
    "        print(submission_df.head().to_string()) # Используем to_string для лучшего форматирования в консоли\n",
    "    except Exception as e:\n",
    "        print(f\"\\n!!! ОШИБКА при сохранении файла submission: {e}\")\n",
    "\n",
    "elif classifier is None:\n",
    "     print(\"\\nПредсказание невозможно: классификатор не был обучен.\")\n",
    "else: # test_embeddings.size == 0\n",
    "    print(\"\\nНет тестовых данных для предсказания или создания файла submission.\")\n",
    "\n",
    "print(\"\\n--- Скрипт завершил работу ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Блок 14: Дополнительный анализ результатов (на обучающих данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Запуск дополнительного анализа ---\")\n",
    "\n",
    "# Создаем обратную карту ID -> Имя автора для меток\n",
    "id_to_author_map = {v: k for k, v in author_to_id_map.items()}\n",
    "author_names = [id_to_author_map[i] for i in sorted(id_to_author_map.keys())] # Упорядоченные имена для меток\n",
    "\n",
    "# --- 1. Матрица ошибок (на основе кросс-валидации) ---\n",
    "print(\"\\n1. Построение матрицы ошибок (кросс-валидация на обучающих данных)...\")\n",
    "\n",
    "if n_splits >= 2: # Кросс-валидация возможна\n",
    "    try:\n",
    "        # Получаем предсказания для каждого чанка с помощью CV\n",
    "        # Используем тот же классификатор, который был выбран (лучший из GridSearchCV или базовый)\n",
    "        # и то же количество фолдов n_splits\n",
    "        y_pred_cv = cross_val_predict(classifier, train_embeddings, train_author_ids, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "        # Строим матрицу ошибок\n",
    "        cm = confusion_matrix(train_author_ids, y_pred_cv, labels=sorted(id_to_author_map.keys()))\n",
    "\n",
    "        # Визуализация матрицы ошибок\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=author_names)\n",
    "        disp.plot(cmap=plt.cm.Blues, xticks_rotation='vertical')\n",
    "        plt.title(f'Матрица ошибок ({n_splits}-fold Cross-Validation)')\n",
    "        plt.tight_layout() # Подгоняем расположение элементов\n",
    "        # Сохраняем график (опционально)\n",
    "        # plt.savefig(\"confusion_matrix_cv.png\")\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   - Ошибка при построении матрицы ошибок: {e}\")\n",
    "        y_pred_cv = None # Сбрасываем предсказания CV, если была ошибка\n",
    "else:\n",
    "    print(\"   - Пропуск построения матрицы ошибок: недостаточно фолдов для cross_val_predict (n_splits < 2).\")\n",
    "    y_pred_cv = None\n",
    "\n",
    "# --- 2. Визуализация эмбеддингов (t-SNE) ---\n",
    "print(\"\\n2. Визуализация эмбеддингов обучающих чанков с помощью t-SNE...\")\n",
    "print(\"   (Может занять некоторое время для большого количества точек)\")\n",
    "\n",
    "# Уменьшим количество точек для ускорения, если их слишком много (например, > 5000)\n",
    "embeddings_to_plot = train_embeddings\n",
    "labels_to_plot = np.array(train_author_ids)\n",
    "texts_to_plot = train_texts\n",
    "indices_to_plot = np.arange(len(labels_to_plot))\n",
    "\n",
    "MAX_POINTS_FOR_TSNE = 5000\n",
    "if len(train_embeddings) > MAX_POINTS_FOR_TSNE:\n",
    "    print(f\"   - Слишком много точек ({len(train_embeddings)}), используется подвыборка из {MAX_POINTS_FOR_TSNE} точек для t-SNE.\")\n",
    "    # Делаем стратифицированную выборку, чтобы сохранить пропорции классов\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    _, sample_indices = train_test_split(\n",
    "        indices_to_plot,\n",
    "        test_size=MAX_POINTS_FOR_TSNE,\n",
    "        stratify=labels_to_plot,\n",
    "        random_state=42\n",
    "    )\n",
    "    embeddings_to_plot = train_embeddings[sample_indices]\n",
    "    labels_to_plot = labels_to_plot[sample_indices]\n",
    "    indices_to_plot = sample_indices # Сохраняем исходные индексы для анализа ошибок\n",
    "\n",
    "try:\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings_to_plot)-1), n_iter=1000, verbose=1) # perplexity не должна превышать N-1\n",
    "    embeddings_2d = tsne.fit_transform(embeddings_to_plot)\n",
    "\n",
    "    # Создание графика\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    unique_labels = sorted(np.unique(labels_to_plot))\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels))) # Используем палитру rainbow\n",
    "\n",
    "    for label, color in zip(unique_labels, colors):\n",
    "        mask = labels_to_plot == label\n",
    "        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1], c=[color], label=id_to_author_map[label], alpha=0.7)\n",
    "\n",
    "    plt.title('Визуализация эмбеддингов обучающих чанков (t-SNE)')\n",
    "    plt.xlabel('Компонента t-SNE 1')\n",
    "    plt.ylabel('Компонента t-SNE 2')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    # Сохраняем график (опционально)\n",
    "    # plt.savefig(\"tsne_embeddings.png\")\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   - Ошибка при построении t-SNE: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Анализ ошибочных предсказаний (на основе кросс-валидации) ---\n",
    "print(\"\\n3. Анализ ошибочных предсказаний (на основе кросс-валидации)...\")\n",
    "\n",
    "if y_pred_cv is not None:\n",
    "    misclassified_indices = np.where(y_pred_cv != train_author_ids)[0]\n",
    "    num_misclassified = len(misclassified_indices)\n",
    "    print(f\"   - Всего ошибок на кросс-валидации: {num_misclassified} из {len(train_author_ids)} ({num_misclassified/len(train_author_ids)*100:.2f}%)\")\n",
    "\n",
    "    if num_misclassified > 0:\n",
    "        print(f\"\\n   Примеры ошибочно классифицированных чанков (до 10):\")\n",
    "        np.random.shuffle(misclassified_indices) # Перемешаем для случайных примеров\n",
    "        for i, index in enumerate(misclassified_indices[:10]): # Показываем не более 10 примеров\n",
    "            true_label_id = train_author_ids[index]\n",
    "            pred_label_id = y_pred_cv[index]\n",
    "            true_author = id_to_author_map[true_label_id]\n",
    "            pred_author = id_to_author_map[pred_label_id]\n",
    "            text_chunk = train_texts[index]\n",
    "\n",
    "            print(f\"\\n   Пример {i+1}:\")\n",
    "            print(f\"     Истинный автор: {true_author} (ID: {true_label_id})\")\n",
    "            print(f\"     Предсказанный автор: {pred_author} (ID: {pred_label_id})\")\n",
    "            print(f\"     Текст чанка: '{text_chunk[:200]}...'\") # Показываем начало чанка\n",
    "    else:\n",
    "        print(\"   - Ошибок на кросс-валидации не найдено!\")\n",
    "else:\n",
    "    print(\"   - Пропуск анализа ошибок: предсказания кросс-валидации недоступны.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Дополнительный анализ завершен ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmslib_clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
